{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ab213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from gym.wrappers import FrameStack\n",
    "from torchvision import transforms as T\n",
    "from gym.spaces import Box\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "282a2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, sigma_init=1.0):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer(\"weight_epsilon\", torch.empty(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.empty(out_features))\n",
    "\n",
    "        self.sigma_init = sigma_init\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.sigma_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.weight_epsilon.normal_()\n",
    "        self.bias_epsilon.normal_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return F.linear(x, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac354364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(QNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            NoisyLinear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, n_actions)\n",
    "        )\n",
    "        # self.fc_adv = nn.Sequential(\n",
    "        #     NoisyLinear(3136, 256),\n",
    "        #     nn.ReLU(),\n",
    "        #     NoisyLinear(256, n_actions)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        val = self.fc_val(x)\n",
    "        # adv = self.fc_adv(x)\n",
    "        # q = val + (adv - adv.mean(dim=1, keepdim=True))\n",
    "        return val\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.len = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.len = min(self.len + 1, self.capacity)\n",
    "\n",
    "        state = torch.tensor(np.array(state).copy(), dtype=torch.float32)\n",
    "        next_state = torch.tensor(np.array(next_state).copy(), dtype=torch.float32)\n",
    "        action = torch.tensor([action], dtype=torch.int64)\n",
    "        reward = torch.tensor([reward], dtype=torch.float32)\n",
    "        done = torch.tensor([done], dtype=torch.float32)\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(torch.stack, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "class DQNVariant:\n",
    "    def __init__(self, action_size):\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 128\n",
    "        self.learn_start = 5000\n",
    "        self.target_update_freq = 1000\n",
    "        self.update_count = 0\n",
    "        self.tau = 1.0\n",
    "        \n",
    "        self.epsilon = 0.2\n",
    "        # self.eps_decay = 0.99999975\n",
    "        # self.eps_min = 0.1\n",
    "        \n",
    "        self.testing = False\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.q_net = QNet(action_size).to(self.device)\n",
    "        self.target_net = QNet(action_size).to(self.device)\n",
    "        self.update(learning=1.0)\n",
    "        for p in self.target_net.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=0.00025)\n",
    "        self.replay_buffer = ReplayBuffer(50000)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        deterministic = True\n",
    "        if(not self.testing):\n",
    "            self.q_net.reset_noise()\n",
    "            deterministic = random.random() > self.epsilon\n",
    "            # self.epsilon *= self.eps_decay\n",
    "            # self.epsilon = max(self.eps_min, self.epsilon)\n",
    "\n",
    "        if(not deterministic): return np.random.randint(self.action_size)\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(np.array(state).copy(), dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_net(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        return action\n",
    "\n",
    "    def update(self, learning):\n",
    "        for target_param, param in zip(self.target_net.parameters(), self.q_net.parameters()):\n",
    "            target_param.data.copy_(learning * param.data + (1 - learning) * target_param.data)\n",
    "\n",
    "    def train(self):\n",
    "        if self.replay_buffer.len < self.learn_start:\n",
    "            return\n",
    "\n",
    "        self.update_count += 1\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        self.q_net.reset_noise()\n",
    "        self.target_net.reset_noise()\n",
    "        \n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.q_net(next_states).argmax(1, keepdim=True)\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     max_next_q = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "        #     target_q = rewards + (1 - dones) * self.gamma * max_next_q\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.target_update_freq == 0:\n",
    "            self.update(learning=self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79aeb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for i in range(self.skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done: break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "# class GrayScaleObservation(gym.ObservationWrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super().__init__(env)\n",
    "#         obs_shape = self.observation_space.shape[:2]\n",
    "#         self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "#     def permute_orientation(self, observation):\n",
    "#         # permute [H, W, C] array to [C, H, W] tensor\n",
    "#         observation = np.transpose(observation, (2, 0, 1))\n",
    "#         observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "#         return observation\n",
    "\n",
    "#     def observation(self, observation):\n",
    "#         observation = self.permute_orientation(observation)\n",
    "#         transform = T.Grayscale()\n",
    "#         observation = transform(observation)\n",
    "#         return observation\n",
    "\n",
    "\n",
    "# class ResizeObservation(gym.ObservationWrapper):\n",
    "#     def __init__(self, env, shape):\n",
    "#         super().__init__(env)\n",
    "#         if isinstance(shape, int):\n",
    "#             self.shape = (shape, shape)\n",
    "#         else:\n",
    "#             self.shape = tuple(shape)\n",
    "\n",
    "#         obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "#         self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "#     def observation(self, observation):\n",
    "#         transforms = T.Compose(\n",
    "#             [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "#         )\n",
    "#         observation = transforms(observation).squeeze(0)\n",
    "#         return observation\n",
    "\n",
    "class TransformObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=255,\n",
    "            shape=(1, *self.shape),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.transform = T.Compose([\n",
    "            T.Grayscale(),\n",
    "            T.Resize(self.shape, antialias=True),\n",
    "            T.Normalize(0, 255)\n",
    "        ])\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        observation = self.transform(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49525f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def test_agent(agent):\n",
    "    sim_env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    sim_env = JoypadSpace(sim_env, COMPLEX_MOVEMENT)\n",
    "    sim_env = SkipFrame(sim_env, skip=4)\n",
    "    sim_env = TransformObservation(sim_env, shape=84)\n",
    "    sim_env = FrameStack(sim_env, num_stack=4)\n",
    "    \n",
    "    agent.testing = True\n",
    "    agent.q_net.eval()\n",
    "    \n",
    "    agent.epsilon = 0.0\n",
    "    \n",
    "    state = sim_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = sim_env.step(action)\n",
    "        done = done or step >= 5e3\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        sim_env.render()\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "\n",
    "    agent.testing = False\n",
    "    agent.q_net.train()\n",
    "    \n",
    "    print(total_reward, step)\n",
    "    sim_env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09fdc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent():    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = TransformObservation(env, shape=84)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "\n",
    "    action_size = env.action_space.n\n",
    "    state_size = env.observation_space.shape\n",
    "    agent = DQNVariant(action_size)\n",
    "    agent.testing = False\n",
    "    agent.q_net.train()\n",
    "\n",
    "    def count_trainable_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    model = agent.q_net\n",
    "    print(f\"Total Trainable Parameters: {count_trainable_parameters(model):,}\")\n",
    "\n",
    "    # checkpoint = torch.load(\"dqn_agent10.pth\", map_location=agent.device)\n",
    "    # agent.q_net.load_state_dict(checkpoint['q_net'])\n",
    "    # agent.target_net.load_state_dict(checkpoint['target_net'])\n",
    "\n",
    "    num_episodes = 3000\n",
    "    reward_history = []\n",
    "    total_frame = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            done = done or step >= 1e3\n",
    "\n",
    "            agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "            agent.train()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "            total_frame += 1\n",
    "\n",
    "        reward_history.append(total_reward)\n",
    "        print(episode, total_reward, agent.epsilon, step)\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}, Avg. Reward: {np.mean(reward_history[-10:])}\")\n",
    "            # test_agent(agent)\n",
    "            torch.save({\n",
    "                'q_net': agent.q_net.state_dict(),\n",
    "                'target_net': agent.target_net.state_dict(),\n",
    "            }, f\"dqn_agent_{episode + 1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c04dd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 3,297,454\n",
      "0 627.0 0.2 1001\n",
      "1 1326.0 0.2 1001\n",
      "2 1912.0 0.2 951\n",
      "3 1442.0 0.2 449\n",
      "4 1011.0 0.2 341\n",
      "5 1484.0 0.2 586\n",
      "6 1392.0 0.2 483\n",
      "7 1899.0 0.2 319\n",
      "8 2429.0 0.2 775\n",
      "9 1930.0 0.2 1001\n",
      "Episode 10, Avg. Reward: 1545.2\n",
      "10 1910.0 0.2 324\n",
      "11 1810.0 0.2 559\n",
      "12 2266.0 0.2 360\n",
      "13 1024.0 0.2 190\n",
      "14 656.0 0.2 1001\n",
      "15 1992.0 0.2 725\n",
      "16 897.0 0.2 1001\n",
      "17 1055.0 0.2 165\n",
      "18 1259.0 0.2 1001\n",
      "19 1095.0 0.2 1001\n",
      "Episode 20, Avg. Reward: 1396.4\n",
      "20 1453.0 0.2 1001\n",
      "21 1280.0 0.2 1001\n",
      "22 1850.0 0.2 329\n",
      "23 658.0 0.2 1001\n",
      "24 1353.0 0.2 1001\n",
      "25 1559.0 0.2 759\n",
      "26 2113.0 0.2 970\n",
      "27 658.0 0.2 1001\n",
      "28 1877.0 0.2 330\n",
      "29 1503.0 0.2 181\n",
      "Episode 30, Avg. Reward: 1430.4\n",
      "30 2126.0 0.2 564\n",
      "31 1606.0 0.2 715\n",
      "32 2482.0 0.2 414\n",
      "33 1106.0 0.2 215\n",
      "34 1367.0 0.2 626\n",
      "35 2484.0 0.2 708\n",
      "36 1439.0 0.2 298\n",
      "37 1427.0 0.2 263\n",
      "38 1537.0 0.2 397\n",
      "39 2026.0 0.2 341\n",
      "Episode 40, Avg. Reward: 1760.0\n",
      "40 1752.0 0.2 383\n",
      "41 1060.0 0.2 168\n",
      "42 2503.0 0.2 390\n",
      "43 2418.0 0.2 402\n",
      "44 2363.0 0.2 298\n",
      "45 1699.0 0.2 249\n",
      "46 2518.0 0.2 412\n",
      "47 3482.0 0.2 553\n",
      "48 2323.0 0.2 356\n",
      "49 2018.0 0.2 341\n",
      "Episode 50, Avg. Reward: 2213.6\n",
      "50 1048.0 0.2 195\n",
      "51 1633.0 0.2 242\n",
      "52 2029.0 0.2 294\n",
      "53 1848.0 0.2 295\n",
      "54 664.0 0.2 117\n",
      "55 1662.0 0.2 235\n",
      "56 2113.0 0.2 317\n",
      "57 2569.0 0.2 511\n",
      "58 1868.0 0.2 267\n",
      "59 2536.0 0.2 410\n",
      "Episode 60, Avg. Reward: 1797.0\n",
      "60 1122.0 0.2 139\n",
      "61 2273.0 0.2 325\n",
      "62 698.0 0.2 111\n",
      "63 1505.0 0.2 249\n",
      "64 2998.0 0.2 441\n",
      "65 1827.0 0.2 316\n",
      "66 2785.0 0.2 585\n",
      "67 1663.0 0.2 274\n",
      "68 2038.0 0.2 341\n",
      "69 2023.0 0.2 284\n",
      "Episode 70, Avg. Reward: 1893.2\n",
      "70 2111.0 0.2 394\n",
      "71 1661.0 0.2 263\n",
      "72 2509.0 0.2 390\n",
      "73 1567.0 0.2 275\n",
      "74 1106.0 0.2 176\n",
      "75 1486.0 0.2 201\n",
      "76 1644.0 0.2 262\n",
      "77 1803.0 0.2 354\n",
      "78 2688.0 0.2 381\n",
      "79 2387.0 0.2 491\n",
      "Episode 80, Avg. Reward: 1896.2\n",
      "80 2645.0 0.2 549\n",
      "81 1974.0 0.2 371\n",
      "82 2494.0 0.2 437\n",
      "83 1648.0 0.2 256\n",
      "84 2458.0 0.2 457\n",
      "85 2567.0 0.2 538\n",
      "86 1657.0 0.2 331\n",
      "87 2459.0 0.2 384\n",
      "88 2264.0 0.2 329\n",
      "89 1538.0 0.2 429\n",
      "Episode 90, Avg. Reward: 2170.4\n",
      "90 1611.0 0.2 322\n",
      "91 1875.0 0.2 334\n",
      "92 1079.0 0.2 175\n",
      "93 2685.0 0.2 448\n",
      "94 1800.0 0.2 293\n",
      "95 2995.0 0.2 567\n",
      "96 1628.0 0.2 258\n",
      "97 686.0 0.2 129\n",
      "98 2523.0 0.2 407\n",
      "99 2335.0 0.2 433\n",
      "Episode 100, Avg. Reward: 1921.7\n",
      "100 1057.0 0.2 144\n",
      "101 1946.0 0.2 355\n",
      "102 1215.0 0.2 239\n",
      "103 2290.0 0.2 462\n",
      "104 1600.0 0.2 361\n",
      "105 1452.0 0.2 292\n",
      "106 1821.0 0.2 317\n",
      "107 2529.0 0.2 395\n",
      "108 1881.0 0.2 299\n",
      "109 2864.0 0.2 461\n",
      "Episode 110, Avg. Reward: 1865.5\n",
      "110 1463.0 0.2 207\n",
      "111 2112.0 0.2 378\n",
      "112 2379.0 0.2 417\n",
      "113 1756.0 0.2 349\n",
      "114 1508.0 0.2 226\n",
      "115 1463.0 0.2 317\n",
      "116 1625.0 0.2 252\n",
      "117 2966.0 0.2 543\n",
      "118 1870.0 0.2 313\n",
      "119 1635.0 0.2 238\n",
      "Episode 120, Avg. Reward: 1877.7\n",
      "120 2714.0 0.2 483\n",
      "121 2807.0 0.2 430\n",
      "122 2240.0 0.2 412\n",
      "123 2036.0 0.2 364\n",
      "124 2218.0 0.2 416\n",
      "125 1886.0 0.2 289\n",
      "126 1829.0 0.2 300\n",
      "127 1986.0 0.2 318\n",
      "128 1982.0 0.2 432\n",
      "129 2250.0 0.2 369\n",
      "Episode 130, Avg. Reward: 2194.8\n",
      "130 1539.0 0.2 328\n",
      "131 2200.0 0.2 406\n",
      "132 1430.0 0.2 247\n",
      "133 1446.0 0.2 318\n",
      "134 2481.0 0.2 537\n",
      "135 1749.0 0.2 696\n",
      "136 2228.0 0.2 347\n",
      "137 1526.0 0.2 222\n",
      "138 1867.0 0.2 406\n",
      "139 1926.0 0.2 307\n",
      "Episode 140, Avg. Reward: 1839.2\n",
      "140 698.0 0.2 126\n",
      "141 2626.0 0.2 660\n",
      "142 2147.0 0.2 405\n",
      "143 2245.0 0.2 379\n",
      "144 1791.0 0.2 254\n",
      "145 1411.0 0.2 237\n",
      "146 2201.0 0.2 407\n",
      "147 2255.0 0.2 320\n",
      "148 1508.0 0.2 190\n",
      "149 1453.0 0.2 229\n",
      "Episode 150, Avg. Reward: 1833.5\n",
      "150 2137.0 0.2 330\n",
      "151 1780.0 0.2 274\n",
      "152 1823.0 0.2 415\n",
      "153 1818.0 0.2 447\n",
      "154 2050.0 0.2 451\n",
      "155 1552.0 0.2 257\n",
      "156 2219.0 0.2 452\n",
      "157 1451.0 0.2 297\n",
      "158 1592.0 0.2 331\n",
      "159 1538.0 0.2 320\n",
      "Episode 160, Avg. Reward: 1796.0\n",
      "160 1455.0 0.2 290\n",
      "161 1958.0 0.2 367\n",
      "162 1970.0 0.2 470\n",
      "163 2018.0 0.2 403\n",
      "164 1529.0 0.2 237\n",
      "165 1578.0 0.2 292\n",
      "166 1484.0 0.2 212\n",
      "167 1710.0 0.2 260\n",
      "168 2248.0 0.2 405\n",
      "169 2302.0 0.2 463\n",
      "Episode 170, Avg. Reward: 1825.2\n",
      "170 1510.0 0.2 255\n",
      "171 1451.0 0.2 322\n",
      "172 1898.0 0.2 344\n",
      "173 1468.0 0.2 184\n",
      "174 1859.0 0.2 471\n",
      "175 1513.0 0.2 344\n",
      "176 1477.0 0.2 325\n",
      "177 676.0 0.2 120\n",
      "178 1444.0 0.2 399\n",
      "179 2008.0 0.2 462\n",
      "Episode 180, Avg. Reward: 1530.4\n",
      "180 2483.0 0.2 451\n",
      "181 1451.0 0.2 239\n",
      "182 1575.0 0.2 401\n",
      "183 1667.0 0.2 302\n",
      "184 1210.0 0.2 207\n",
      "185 2262.0 0.2 527\n",
      "186 2062.0 0.2 447\n",
      "187 1772.0 0.2 380\n",
      "188 2129.0 0.2 430\n",
      "189 657.0 0.2 130\n",
      "Episode 190, Avg. Reward: 1726.8\n",
      "190 2000.0 0.2 391\n",
      "191 1875.0 0.2 402\n",
      "192 1596.0 0.2 336\n",
      "193 1469.0 0.2 228\n",
      "194 1497.0 0.2 291\n",
      "195 2042.0 0.2 478\n",
      "196 1806.0 0.2 236\n",
      "197 1525.0 0.2 275\n",
      "198 1612.0 0.2 262\n",
      "199 1811.0 0.2 278\n",
      "Episode 200, Avg. Reward: 1723.3\n",
      "200 2029.0 0.2 384\n",
      "201 1444.0 0.2 300\n",
      "202 3047.0 0.2 697\n",
      "203 1239.0 0.2 1001\n",
      "204 3087.0 0.2 514\n",
      "205 2529.0 0.2 515\n",
      "206 2465.0 0.2 462\n",
      "207 1468.0 0.2 274\n",
      "208 1482.0 0.2 262\n",
      "209 1149.0 0.2 238\n",
      "Episode 210, Avg. Reward: 1993.9\n",
      "210 666.0 0.2 144\n",
      "211 1847.0 0.2 372\n",
      "212 1442.0 0.2 234\n",
      "213 1631.0 0.2 253\n",
      "214 1481.0 0.2 303\n",
      "215 2308.0 0.2 645\n",
      "216 2045.0 0.2 382\n",
      "217 1058.0 0.2 177\n",
      "218 1651.0 0.2 273\n",
      "219 1951.0 0.2 555\n",
      "Episode 220, Avg. Reward: 1608.0\n",
      "220 1933.0 0.2 332\n",
      "221 2623.0 0.2 740\n",
      "222 1095.0 0.2 160\n",
      "223 1850.0 0.2 312\n",
      "224 1573.0 0.2 323\n",
      "225 1628.0 0.2 297\n",
      "226 1626.0 0.2 330\n",
      "227 2521.0 0.2 452\n",
      "228 1491.0 0.2 235\n",
      "229 1849.0 0.2 251\n",
      "Episode 230, Avg. Reward: 1818.9\n",
      "230 1899.0 0.2 320\n",
      "231 680.0 0.2 149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[48], line 42\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m done \u001b[38;5;241m=\u001b[39m done \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e3\u001b[39m\n\u001b[0;32m     41\u001b[0m agent\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd(state, action, reward, next_state, done)\n\u001b[1;32m---> 42\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     45\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[42], line 128\u001b[0m, in \u001b[0;36mDQNVariant.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net\u001b[38;5;241m.\u001b[39mreset_noise()\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mreset_noise()\n\u001b[1;32m--> 128\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m#     next_actions = self.q_net(next_states).argmax(1, keepdim=True)\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m#     next_q_values = self.target_net(next_states).gather(1, next_actions)\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m#     target_q = rewards + (1 - dones) * self.gamma * next_q_values\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 31\u001b[0m, in \u001b[0;36mQNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_val(x)\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\activation.py:104\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\fx\\traceback.py:72\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 362\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32mc:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\linecache.py:74\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c66a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_29660\\1094559644.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"dqn_agent_230.pth\", map_location=agent.device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m agent\u001b[38;5;241m.\u001b[39mq_net\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_net\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m agent\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_net\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtest_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 30\u001b[0m, in \u001b[0;36mtest_agent\u001b[1;34m(agent)\u001b[0m\n\u001b[0;32m     27\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     28\u001b[0m     sim_env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m agent\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     33\u001b[0m agent\u001b[38;5;241m.\u001b[39mq_net\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQNVariant(12)\n",
    "\n",
    "checkpoint = torch.load(\"dqn_agent_230.pth\", map_location=agent.device)\n",
    "agent.q_net.load_state_dict(checkpoint['q_net'])\n",
    "agent.target_net.load_state_dict(checkpoint['target_net'])\n",
    "\n",
    "test_agent(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afb4ba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 1,690,284\n",
      "0 446.0 0.9958805064246858 4128\n",
      "Interrupted! Profiling results up to this point:\n",
      "         3390943 function calls (3267985 primitive calls) in 40.783 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 536 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.080    0.080   40.783   40.783 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\1265275117.py:1(train_agent)\n",
      "     4257    0.024    0.000   27.975    0.007 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\gym\\wrappers\\frame_stack.py:116(step)\n",
      "     4257    0.018    0.000   27.912    0.007 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\gym\\core.py:313(step)\n",
      "     4257    0.039    0.000   24.259    0.006 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\629983895.py:6(step)\n",
      "    17027    0.020    0.000   24.221    0.001 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\nes_py\\wrappers\\joypad_space.py:58(step)\n",
      "    17027    0.035    0.000   24.201    0.001 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\gym\\wrappers\\time_limit.py:16(step)\n",
      "    17027    0.020    0.000   24.165    0.001 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:11(step)\n",
      "    17027   21.920    0.001   24.146    0.001 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\nes_py\\nes_env.py:279(step)\n",
      "     4257    0.139    0.000   10.948    0.003 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\814641396.py:115(train)\n",
      "62667/13553    0.067    0.000    4.299    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1549(_wrapped_call_impl)\n",
      "62667/13553    0.137    0.000    4.276    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1555(_call_impl)\n",
      "    10571    3.815    0.000    3.815    0.000 {method 'to' of 'torch._C.TensorBase' objects}\n",
      "     4259    0.380    0.000    3.638    0.001 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\629983895.py:29(observation)\n",
      "     1258    2.428    0.002    2.555    0.002 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\814641396.py:65(sample)\n",
      "     4259    0.030    0.000    2.328    0.001 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:93(__call__)\n",
      "     3778    0.070    0.000    1.872    0.000 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\814641396.py:38(forward)\n",
      "     4259    0.012    0.000    1.807    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:346(forward)\n",
      "     4259    0.048    0.000    1.796    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torchvision\\transforms\\functional.py:387(resize)\n",
      "     4259    0.035    0.000    1.605    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:439(resize)\n",
      "    15112    0.073    0.000    1.548    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217(forward)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "try:\n",
    "    profiler.enable()\n",
    "    train_agent()  # long-running code\n",
    "    profiler.disable()\n",
    "except KeyboardInterrupt:\n",
    "    profiler.disable()\n",
    "    print(\"Interrupted! Profiling results up to this point:\")\n",
    "\n",
    "s = io.StringIO()\n",
    "ps = pstats.Stats(profiler, stream=s).sort_stats('cumtime')\n",
    "ps.print_stats(20)\n",
    "print(s.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
