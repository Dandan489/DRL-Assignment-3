{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77ab213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from gym.wrappers import FrameStack\n",
    "from torchvision import transforms as T\n",
    "from gym.spaces import Box\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282a2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer(\"weight_epsilon\", torch.empty(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.empty(out_features))\n",
    "\n",
    "        self.sigma_init = sigma_init\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.sigma_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.weight_epsilon.normal_()\n",
    "        self.bias_epsilon.normal_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return F.linear(x, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac354364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(QNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            NoisyLinear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, 1)\n",
    "        )\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            NoisyLinear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        val = self.fc_val(x)\n",
    "        adv = self.fc_adv(x)\n",
    "        q = val + (adv - adv.mean(dim=1, keepdim=True))\n",
    "        return q\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, n_step=3, gamma=0.99, alpha=0.5):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.n_step = n_step\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        \n",
    "        self.pos = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done, episode_end=False):\n",
    "        state = torch.tensor(np.array(state).copy(), dtype=torch.float32)\n",
    "        next_state = torch.tensor(np.array(next_state).copy(), dtype=torch.float32)\n",
    "        action = torch.tensor([action], dtype=torch.int64)\n",
    "        reward = torch.tensor([reward], dtype=torch.float32)\n",
    "        done = torch.tensor([done], dtype=torch.float32)\n",
    "        \n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return\n",
    "        \n",
    "        state_n, action_n, reward_n, next_state_n, done_n = self._get_n_step_info()\n",
    "        \n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state_n, action_n, reward_n, next_state_n, done_n))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state_n, action_n, reward_n, next_state_n, done_n)\n",
    "        \n",
    "        self.priorities[self.pos] = max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        \n",
    "        if episode_end or done.item() == 1:\n",
    "            while len(self.n_step_buffer) > 0:\n",
    "                state_n, action_n, reward_n, next_state_n, done_n = self._get_n_step_info()\n",
    "                \n",
    "                if len(self.buffer) < self.capacity:\n",
    "                    self.buffer.append((state_n, action_n, reward_n, next_state_n, done_n))\n",
    "                else:\n",
    "                    self.buffer[self.pos] = (state_n, action_n, reward_n, next_state_n, done_n)\n",
    "                \n",
    "                self.priorities[self.pos] = max_priority\n",
    "                self.pos = (self.pos + 1) % self.capacity\n",
    "                \n",
    "                self.n_step_buffer.popleft()\n",
    "            \n",
    "            self.n_step_buffer.clear()\n",
    "    \n",
    "    def _get_n_step_info(self):\n",
    "        reward, next_state, done = 0.0, None, None\n",
    "        for idx, (_, _, r, next_s, d) in enumerate(self.n_step_buffer):\n",
    "            reward += (self.gamma ** idx) * r\n",
    "            next_state = next_s\n",
    "            done = d\n",
    "            if d.item() == 1:\n",
    "                break\n",
    "        state, action, _, _, _ = self.n_step_buffer[0]\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            probs = self.priorities\n",
    "        else:\n",
    "            probs = self.priorities[:self.pos]\n",
    "        \n",
    "        probs = probs ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = torch.tensor(weights, dtype=torch.float32)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = map(torch.stack, zip(*samples))\n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "class DQNVariant:\n",
    "    def __init__(self, action_size):\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.learn_start = 10000\n",
    "        # self.target_update_freq = 1000\n",
    "        self.update_count = 0\n",
    "        self.tau = 0.005\n",
    "        \n",
    "        # self.epsilon = 0.2\n",
    "        # self.eps_decay = 0.99999975\n",
    "        # self.eps_min = 0.1\n",
    "        \n",
    "        self.testing = False\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.q_net = QNet(action_size).to(self.device)\n",
    "        self.target_net = QNet(action_size).to(self.device)\n",
    "        self.update(learning=1.0)\n",
    "        for p in self.target_net.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=0.00025)\n",
    "        self.replay_buffer = ReplayBuffer(100000)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        deterministic = True\n",
    "        if(not self.testing):\n",
    "            self.q_net.reset_noise()\n",
    "            # deterministic = random.random() > self.epsilon\n",
    "            # self.epsilon *= self.eps_decay\n",
    "            # self.epsilon = max(self.eps_min, self.epsilon)\n",
    "\n",
    "        if(not deterministic): return np.random.randint(self.action_size)\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(np.array(state).copy(), dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_net(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        return action\n",
    "\n",
    "    def update(self, learning):\n",
    "        for target_param, param in zip(self.target_net.parameters(), self.q_net.parameters()):\n",
    "            target_param.data.copy_(learning * param.data + (1 - learning) * target_param.data)\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer.buffer) < self.learn_start:\n",
    "            return\n",
    "        \n",
    "        beta = min(0.4 + (self.update_count / 2e6), 1.0)\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.replay_buffer.sample(self.batch_size, beta)\n",
    "\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        weights = weights.to(self.device)\n",
    "        \n",
    "        self.q_net.reset_noise()\n",
    "        self.target_net.reset_noise()\n",
    "        \n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.q_net(next_states).argmax(1, keepdim=True)\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        td_errors = (q_values - target_q).squeeze(1)\n",
    "        loss = F.smooth_l1_loss(q_values, target_q, reduction='none').squeeze(1)\n",
    "        loss = (weights.to(self.device) * loss).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.replay_buffer.update_priorities(indices, (td_errors.abs().cpu().detach().numpy() + 1e-6))\n",
    "        \n",
    "        self.update_count += 1\n",
    "        self.update(learning=self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79aeb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for i in range(self.skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done: break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "# class GrayScaleObservation(gym.ObservationWrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super().__init__(env)\n",
    "#         obs_shape = self.observation_space.shape[:2]\n",
    "#         self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "#     def permute_orientation(self, observation):\n",
    "#         # permute [H, W, C] array to [C, H, W] tensor\n",
    "#         observation = np.transpose(observation, (2, 0, 1))\n",
    "#         observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "#         return observation\n",
    "\n",
    "#     def observation(self, observation):\n",
    "#         observation = self.permute_orientation(observation)\n",
    "#         transform = T.Grayscale()\n",
    "#         observation = transform(observation)\n",
    "#         return observation\n",
    "\n",
    "\n",
    "# class ResizeObservation(gym.ObservationWrapper):\n",
    "#     def __init__(self, env, shape):\n",
    "#         super().__init__(env)\n",
    "#         if isinstance(shape, int):\n",
    "#             self.shape = (shape, shape)\n",
    "#         else:\n",
    "#             self.shape = tuple(shape)\n",
    "\n",
    "#         obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "#         self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "#     def observation(self, observation):\n",
    "#         transforms = T.Compose(\n",
    "#             [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "#         )\n",
    "#         observation = transforms(observation).squeeze(0)\n",
    "#         return observation\n",
    "\n",
    "class TransformObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=255,\n",
    "            shape=(1, *self.shape),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.transform = T.Compose([\n",
    "            T.Grayscale(),\n",
    "            T.Resize(self.shape, antialias=True),\n",
    "            T.Normalize(0, 255)\n",
    "        ])\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        observation = self.transform(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49525f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "def test_agent(agent):\n",
    "    sim_env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    sim_env = JoypadSpace(sim_env, COMPLEX_MOVEMENT)\n",
    "    sim_env = SkipFrame(sim_env, skip=4)\n",
    "    sim_env = TransformObservation(sim_env, shape=84)\n",
    "    sim_env = FrameStack(sim_env, num_stack=4)\n",
    "    \n",
    "    agent.testing = True\n",
    "    agent.q_net.eval()\n",
    "    \n",
    "    # agent.epsilon = 0.0\n",
    "    \n",
    "    state = sim_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = sim_env.step(action)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        sim_env.render()\n",
    "        \n",
    "        time.sleep(0.02)\n",
    "\n",
    "    agent.testing = False\n",
    "    agent.q_net.train()\n",
    "    \n",
    "    print(total_reward, step)\n",
    "    sim_env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09fdc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent():    \n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = TransformObservation(env, shape=84)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "\n",
    "    action_size = env.action_space.n\n",
    "    state_size = env.observation_space.shape\n",
    "    agent = DQNVariant(action_size)\n",
    "    agent.testing = False\n",
    "    agent.q_net.train()\n",
    "\n",
    "    # def count_trainable_parameters(model):\n",
    "    #     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # model = agent.q_net\n",
    "    # print(f\"Total Trainable Parameters: {count_trainable_parameters(model):,}\")\n",
    "\n",
    "    checkpoint = torch.load(\"4th/dqn_agent_1770.pth\", map_location=agent.device)\n",
    "    agent.q_net.load_state_dict(checkpoint['q_net'])\n",
    "    agent.target_net.load_state_dict(checkpoint['target_net'])\n",
    "    # agent.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    num_episodes = 3000\n",
    "    reward_history = []\n",
    "    total_frame = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        last_life = 2\n",
    "        last_xpos = 0\n",
    "        info = None\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            done = done or info[\"life\"] < last_life\n",
    "            clipped_reward = np.clip(reward, -1, 1)\n",
    "            \n",
    "            agent.replay_buffer.add(state, action, clipped_reward, next_state, done)\n",
    "            agent.train()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "            total_frame += 1\n",
    "            if not done: last_xpos = info[\"x_pos\"]\n",
    "            \n",
    "            # print(step)\n",
    "\n",
    "        reward_history.append(total_reward)\n",
    "        print(episode, total_reward, step, last_xpos)\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}, Avg. Reward: {np.mean(reward_history[-10:])}\")\n",
    "            # test_agent(agent)\n",
    "            torch.save({\n",
    "                'q_net': agent.q_net.state_dict(),\n",
    "                'target_net': agent.target_net.state_dict(),\n",
    "                'optimizer': agent.optimizer.state_dict(),\n",
    "            }, f\"dqn_agent_{episode + 1}.pth\")\n",
    "            # for name, module in agent.q_net.named_modules():\n",
    "            #     if isinstance(module, NoisyLinear):\n",
    "            #         print(f\"Layer: {name}\")\n",
    "            #         print(f\"  weight_sigma mean: {module.weight_sigma.mean().item():.6f}\")\n",
    "            #         print(f\"  bias_sigma mean:   {module.bias_sigma.mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c04dd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_18608\\4073198778.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"4th/dqn_agent_1770.pth\", map_location=agent.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2773.0 2391 2227\n",
      "1 1112.0 279 1222\n",
      "2 797.0 225 895\n",
      "3 2517.0 655 1624\n",
      "4 1522.0 334 1642\n",
      "5 2132.0 566 1219\n",
      "6 2665.0 744 1779\n",
      "7 1260.0 285 290\n",
      "8 1373.0 340 1496\n",
      "9 1277.0 360 1402\n",
      "Episode 10, Avg. Reward: 1742.8\n",
      "10 1388.0 323 1509\n",
      "11 2053.0 531 1128\n",
      "12 1652.0 380 1785\n",
      "13 2521.0 632 1621\n",
      "14 1102.0 322 1222\n",
      "15 2120.0 596 1220\n",
      "16 2124.0 586 1213\n",
      "17 2323.0 888 1384\n",
      "18 987.0 381 1117\n",
      "19 714.0 355 837\n",
      "Episode 20, Avg. Reward: 1698.4\n",
      "20 1087.0 389 1218\n",
      "21 220.0 44 284\n",
      "22 582.0 160 669\n",
      "23 221.0 45 285\n",
      "24 671.0 2005 226\n",
      "25 942.0 872 2699\n",
      "26 202.0 51 268\n",
      "27 706.0 203 802\n",
      "28 633.0 171 722\n",
      "29 2053.0 2300 1495\n",
      "Episode 30, Avg. Reward: 731.7\n",
      "30 982.0 471 1133\n",
      "31 1535.0 367 1665\n",
      "32 702.0 185 796\n",
      "33 1478.0 404 1613\n",
      "34 669.0 2001 226\n",
      "35 646.0 2004 2628\n",
      "36 455.0 2002 15\n",
      "37 754.0 479 898\n",
      "38 676.0 278 786\n",
      "39 1103.0 311 1218\n",
      "Episode 40, Avg. Reward: 900.0\n",
      "40 615.0 189 706\n",
      "41 726.0 177 815\n",
      "42 482.0 2002 44\n",
      "43 976.0 294 1087\n",
      "44 1292.0 315 1409\n",
      "45 456.0 2001 14\n",
      "46 764.0 222 865\n",
      "47 219.0 45 285\n",
      "48 573.0 142 654\n",
      "49 976.0 351 1101\n",
      "Episode 50, Avg. Reward: 707.9\n",
      "50 772.0 212 871\n",
      "51 1996.0 624 1102\n",
      "52 538.0 2005 993\n",
      "53 227.0 42 289\n",
      "54 1582.0 488 661\n",
      "55 452.0 2004 15\n",
      "56 720.0 203 815\n",
      "57 747.0 205 842\n",
      "58 942.0 546 1102\n",
      "59 1513.0 403 1649\n",
      "Episode 60, Avg. Reward: 948.9\n",
      "60 1458.0 416 1596\n",
      "61 1019.0 295 1131\n",
      "62 804.0 199 898\n",
      "63 1594.0 486 665\n",
      "64 1643.0 410 1779\n",
      "65 1700.0 453 680\n",
      "66 226.0 43 289\n",
      "67 758.0 178 851\n",
      "68 581.0 2003 2560\n",
      "69 2257.0 835 1409\n",
      "Episode 70, Avg. Reward: 1204.0\n",
      "70 794.0 196 888\n",
      "71 805.0 192 898\n",
      "72 1238.0 388 290\n",
      "73 1410.0 439 1550\n",
      "74 2732.0 734 1780\n",
      "75 620.0 169 707\n",
      "76 633.0 175 722\n",
      "77 1094.0 356 1221\n",
      "78 1648.0 418 722\n",
      "79 800.0 217 898\n",
      "Episode 80, Avg. Reward: 1177.4\n",
      "80 220.0 43 283\n",
      "81 1090.0 348 1216\n",
      "82 1479.0 329 1597\n",
      "83 2030.0 574 1118\n",
      "84 1024.0 215 1119\n",
      "85 225.0 40 287\n",
      "86 110.0 84 181\n",
      "87 1113.0 282 1226\n",
      "88 1119.0 255 1226\n",
      "89 1356.0 430 1496\n",
      "Episode 90, Avg. Reward: 976.6\n",
      "90 1751.0 2005 2206\n",
      "91 1379.0 600 1396\n",
      "92 1227.0 369 283\n",
      "93 235.0 39 298\n",
      "94 651.0 302 766\n",
      "95 1826.0 451 898\n",
      "96 228.0 41 291\n",
      "97 234.0 39 297\n",
      "98 491.0 2002 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m clipped_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(reward, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     45\u001b[0m agent\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd(state, action, clipped_reward, next_state, done)\n\u001b[1;32m---> 46\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     49\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[3], line 207\u001b[0m, in \u001b[0;36mDQNVariant.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mupdate_priorities(indices, (\u001b[43mtd_errors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m))\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(learning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c66a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_17820\\4161182934.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"dqn_agent_10.pth\", map_location=agent.device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m agent\u001b[38;5;241m.\u001b[39mq_net\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_net\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m agent\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_net\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtest_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 30\u001b[0m, in \u001b[0;36mtest_agent\u001b[1;34m(agent)\u001b[0m\n\u001b[0;32m     27\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     28\u001b[0m     sim_env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m agent\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     33\u001b[0m agent\u001b[38;5;241m.\u001b[39mq_net\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQNVariant(12)\n",
    "\n",
    "checkpoint = torch.load(\"4th/dqn_agent_1770.pth\", map_location=agent.device)\n",
    "agent.q_net.load_state_dict(checkpoint['q_net'])\n",
    "agent.target_net.load_state_dict(checkpoint['target_net'])\n",
    "\n",
    "test_agent(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f43f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next_state.shape)\n",
    "# cv2.imshow(\"Next State\", np.array(next_state[0]))\n",
    "# cv2.waitKey(10000)\n",
    "# with open(\"dump.txt\", \"w+\") as f:\n",
    "#     torch.set_printoptions(threshold=float('inf'))\n",
    "#     print(next_state[0], file=f)\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afb4ba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 1,690,284\n",
      "0 446.0 0.9958805064246858 4128\n",
      "Interrupted! Profiling results up to this point:\n",
      "         3390943 function calls (3267985 primitive calls) in 40.783 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 536 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.080    0.080   40.783   40.783 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\1265275117.py:1(train_agent)\n",
      "     4257    0.024    0.000   27.975    0.007 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\gym\\wrappers\\frame_stack.py:116(step)\n",
      "     4257    0.018    0.000   27.912    0.007 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\gym\\core.py:313(step)\n",
      "     4257    0.039    0.000   24.259    0.006 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\629983895.py:6(step)\n",
      "    17027    0.020    0.000   24.221    0.001 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\nes_py\\wrappers\\joypad_space.py:58(step)\n",
      "    17027    0.035    0.000   24.201    0.001 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\gym\\wrappers\\time_limit.py:16(step)\n",
      "    17027    0.020    0.000   24.165    0.001 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:11(step)\n",
      "    17027   21.920    0.001   24.146    0.001 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\nes_py\\nes_env.py:279(step)\n",
      "     4257    0.139    0.000   10.948    0.003 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\814641396.py:115(train)\n",
      "62667/13553    0.067    0.000    4.299    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1549(_wrapped_call_impl)\n",
      "62667/13553    0.137    0.000    4.276    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1555(_call_impl)\n",
      "    10571    3.815    0.000    3.815    0.000 {method 'to' of 'torch._C.TensorBase' objects}\n",
      "     4259    0.380    0.000    3.638    0.001 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\629983895.py:29(observation)\n",
      "     1258    2.428    0.002    2.555    0.002 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\814641396.py:65(sample)\n",
      "     4259    0.030    0.000    2.328    0.001 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:93(__call__)\n",
      "     3778    0.070    0.000    1.872    0.000 C:\\Users\\Danniel\\AppData\\Local\\Temp\\ipykernel_23600\\814641396.py:38(forward)\n",
      "     4259    0.012    0.000    1.807    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:346(forward)\n",
      "     4259    0.048    0.000    1.796    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torchvision\\transforms\\functional.py:387(resize)\n",
      "     4259    0.035    0.000    1.605    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:439(resize)\n",
      "    15112    0.073    0.000    1.548    0.000 c:\\Users\\Danniel\\anaconda3\\envs\\drl-hw3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217(forward)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "try:\n",
    "    profiler.enable()\n",
    "    train_agent()  # long-running code\n",
    "    profiler.disable()\n",
    "except KeyboardInterrupt:\n",
    "    profiler.disable()\n",
    "    print(\"Interrupted! Profiling results up to this point:\")\n",
    "\n",
    "s = io.StringIO()\n",
    "ps = pstats.Stats(profiler, stream=s).sort_stats('cumtime')\n",
    "ps.print_stats(20)\n",
    "print(s.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
